{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_ Video.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncPer7MulwaY"
      },
      "source": [
        "**Connecting Drive to Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "while 1:\n",
        "    a.append(\"1\")"
      ],
      "metadata": {
        "id": "DH8EgQ-Acx9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FkprWXdn7N3",
        "outputId": "597e534b-bcc7-44ec-aa46-8660c8ccbf0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processData(data):\n",
        "    X = data.iloc[:,:].values\n",
        "    X = np.delete(X, 0, 1)\n",
        "    X = np.delete(X, 1, 1)\n",
        "    for i in range(len(X)):\n",
        "        if(isinstance(X[i][5],str) or isinstance(X[i][7],str)):\n",
        "            X[i] = np.zeros((1, X.shape[1]))\n",
        "            # print(\"se\" , end = \" \")\n",
        "    return X\n",
        "\n",
        "def getData(filename):\n",
        "    data = pd.read_csv(filename,delimiter=',', engine='python')\n",
        "    X = processData(data)\n",
        "    return X\n",
        "\n",
        "def makeDataPoint(patientNo):\n",
        "    file1 = (patientNo)+\"_CLNF_AUs.txt\"\n",
        "    file2 = (patientNo)+\"_CLNF_features.txt\"\n",
        "    file3 = (patientNo)+\"_CLNF_features3D.txt\"\n",
        "    file4 = (patientNo)+\"_CLNF_gaze.txt\"\n",
        "    file5 = (patientNo)+\"_CLNF_hog.txt\"\n",
        "    file6 = (patientNo)+\"_CLNF_pose.txt\"\n",
        "\n",
        "    X1 = getData(file1)\n",
        "    X2 = getData(file2)\n",
        "    X3 = getData(file3)\n",
        "    X4 = getData(file4)\n",
        "    X6 = getData(file6)\n",
        "\n",
        "    X = np.concatenate((X1, X2, X3, X4, X6), 1)\n",
        "    return X"
      ],
      "metadata": {
        "id": "WeueHnlKSljp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_down(X):\n",
        "  X_new = []\n",
        "  size = 2\n",
        "  for i in range(int(X.shape[0]/size)):\n",
        "    cur_row = X[i*size]\n",
        "    for j in range(1,size):\n",
        "      if(i+j < X.shape[0]):\n",
        "        cur_row += X[i+j]\n",
        "    cur_row = cur_row/size\n",
        "    X_new.append(cur_row)\n",
        "  X_new = np.array(X_new)\n",
        "  return X_new\n",
        "\n",
        "def decrease_size(X):\n",
        "  size = 10000\n",
        "  if(X.shape[0] < size):\n",
        "    dif = size - X.shape[0] \n",
        "    temp = np.zeros((dif,X.shape[1]))\n",
        "    X = np.concatenate((X,temp),axis = 0)\n",
        "  elif(X.shape[0] > size):\n",
        "    X = X[:10000, :]\n",
        "  return X\n",
        "\n",
        "def makeDataset(location, folder):\n",
        "    file  = np.array(pd.read_csv(location,delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "    X_temp = []\n",
        "    Y_temp = []\n",
        "    for i in range(len(file)):\n",
        "      patientID = (str(int(file[i][0])))\n",
        "      string = '/content/drive/My Drive/DAIC/' + folder +\"/\"+ patientID\n",
        "      XT = makeDataPoint(string)\n",
        "      XT = scale_down(XT)\n",
        "      XT = decrease_size(XT)\n",
        "      X_temp.append(XT)\n",
        "      Y_temp.append(int(file[i][1]))\n",
        "    Y_temp = np.asarray(Y_temp)\n",
        "    return X_temp, Y_temp"
      ],
      "metadata": {
        "id": "7vbM8TRGScf0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzxb5JsroQ5y"
      },
      "source": [
        "class CNN_Video:\n",
        "  def __init__(self):\n",
        "    # Initialising the CNN\n",
        "    classifier = Sequential()\n",
        "    # Step 1 - Convolution\n",
        "    classifier.add(Conv1D(300, 3, input_shape = (10000, 388), activation = 'relu'))\n",
        "    # Step 2 - Pooling\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    classifier.add(Conv1D(150, 3, activation = 'relu'))\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    # Adding a second convolutional layer\n",
        "    classifier.add(Conv1D(75, 3, activation = 'relu'))\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    classifier.add(Conv1D(32, 3, activation = 'relu'))\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    # Step 3 - Flattening\n",
        "    classifier.add(Flatten())\n",
        "    # Step 4 - Full connection\n",
        "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "  # Compiling the CNN\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
        "    class_weight = { 0: 0.3, 1:0.7}\n",
        "    self.classifier = classifier\n",
        "\n",
        "  def fitModel(self, XtrainTotal, YtrainTotal, epoch = 5):\n",
        "    return self.classifier.fit(XtrainTotal, YtrainTotal, epochs=epoch)\n",
        "\n",
        "  def predictModel(self, X_test):\n",
        "    return self.classifier.predict(np.asarray(X_test))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClUVyIzMXre8",
        "outputId": "e22d39ee-b9e9-4107-b648-7bee5b8dd22a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# # Get Dataset\n",
        "X_train, Y_train = makeDataset('/content/drive/My Drive/DAIC/train_split_Depression_AVEC2017.csv', 'train_data')\n",
        "X_dev, Y_dev = makeDataset('/content/drive/My Drive/DAIC/dev_split_Depression_AVEC2017.csv', 'dev_data')\n",
        "X_test, Y_test = makeDataset('/content/drive/My Drive/DAIC/full_test_split.csv', 'test_data')\n",
        "YtrainTotal = np.concatenate((Y_train, Y_dev))\n",
        "XtrainTotal = X_train + X_dev \n",
        "XtrainTotal = np.asarray(XtrainTotal, dtype=np.float32)\n",
        "\n",
        "# # Run Model On Test Set\n",
        "model = CNN_Video()\n",
        "model.fitModel(XtrainTotal, YtrainTotal, 5)\n",
        "Y_pred = model.predictModel(X_test)\n",
        "print(classification_report(Y_test,Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "5/5 [==============================] - 31s 6s/step - loss: 98845.8516\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 30s 6s/step - loss: 4331.5200\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 29s 6s/step - loss: 5901.1006\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 29s 6s/step - loss: 9088.0293\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 29s 6s/step - loss: 2667.6003\n"
          ]
        }
      ]
    }
  ]
}