{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_With_Gating_Sentence_Level.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJLN_Guyi9E8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJyWRMcLCjbB",
        "outputId": "16d9068b-7c0b-43f9-8f54-8b027f13e8d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pclNO84uR_0",
        "outputId": "e56b54b5-0486-48d7-d947-9ae9d986adcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gc\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import math\n",
        "\n",
        "from smart_open import open\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import concatenate\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzWbjnf8EruV"
      },
      "source": [
        "#Text + Audio \n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class Highway(layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Highway, self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    n_sentences = input_shape[1]\n",
        "    n_features = input_shape[2]\n",
        "    carry_bias = keras.initializers.Constant(value=-1.0)\n",
        "    random_dist = keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
        "\n",
        "    carry_bias_2 = keras.initializers.Constant(value= 0.1)\n",
        "\n",
        "    self.W_T = self.add_weight(shape=(n_features, n_features),initializer = random_dist,trainable=True)\n",
        "    self.b_T = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias, trainable=True)\n",
        "    # self.W_T = tf.Variable(tf.random.truncated_normal([n_data_points,n_features, n_features], stddev=0.1),trainable=True)\n",
        "    # self.b_T = tf.Variable(tf.constant(-1.0, shape=[n_data_points, n_sentences, n_features]),trainable=True)\n",
        "   \n",
        "    self.W = self.add_weight(shape=( n_features, n_features),initializer = random_dist, trainable=True)\n",
        "    self.b = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias_2, trainable=True)\n",
        "    # self.W = tf.Variable(tf.random.truncated_normal([n_data_points,n_features, n_features], stddev=0.1), trainable=True)\n",
        "    # self.b = tf.Variable(tf.constant(0.1, shape=[n_data_points, n_sentences, n_features]), trainable=True)  \n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    T = tf.sigmoid(tf.matmul(x, self.W_T) + self.b_T, name=\"transform_gate\")\n",
        "    H = tf.nn.relu(tf.matmul(x, self.W) + self.b, name=\"activation\")\n",
        "    C = tf.subtract(1.0, T, name=\"carry_gate\")\n",
        "    \n",
        "    return tf.add(tf.multiply(H, T), tf.multiply(x, C), \"y\")\n",
        "\n",
        "\n",
        "# Multiple Inputs\n",
        "class text_audio:\n",
        "  # first input model\n",
        "  input1 = Input(shape=(250,74), name = 'Audio_input')\n",
        "  highway1 = Highway()(input1)\n",
        "  highway5 = Highway()(highway1)\n",
        "  highway6 = Highway()(highway5)\n",
        "  dense1 = Dense(74)(highway6)\n",
        "\n",
        "  input3 = Input(shape = (250,5100), name = 'Text_input')\n",
        "  dense4 = Dense(1000)(input3)\n",
        "  dense5 = Dense(500)(dense4)\n",
        "  dense6 = Dense(250)(dense5)\n",
        "  dense3 = Dense(74)(dense6)\n",
        "  # merge input models\n",
        "  merge = concatenate([dense1,dense3], axis = 1)\n",
        "  # interpretation model\n",
        "  lstm = LSTM(128, dropout = 0.2, recurrent_dropout = 0.2)(merge)\n",
        "  output = Dense(1, activation='sigmoid')(lstm)\n",
        "  model = Model(inputs=[input1, input3], outputs=output)\n",
        "  # summarize layers\n",
        "  # print(model.summary())\n",
        "  # plot graph\n",
        "  # plot_model(model)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate= 0.0001)\n",
        "  def run_model(self):\n",
        "    self.model.compile(optimizer=self.optimizer, loss='binary_crossentropy')\n",
        "\n",
        "    return self.model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsHsKkisFMM7"
      },
      "source": [
        "#Text+Video\n",
        "\n",
        "\n",
        "class Highway(layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Highway, self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    n_sentences = input_shape[1]\n",
        "    n_features = input_shape[2]\n",
        "    carry_bias = keras.initializers.Constant(value=-1.0)\n",
        "    random_dist = keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
        "\n",
        "    carry_bias_2 = keras.initializers.Constant(value= 0.1)\n",
        "\n",
        "    self.W_T = self.add_weight(shape=(n_features, n_features),initializer = random_dist,trainable=True)\n",
        "    self.b_T = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias, trainable=True)\n",
        "   \n",
        "    self.W = self.add_weight(shape=( n_features, n_features),initializer = random_dist, trainable=True)\n",
        "    self.b = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias_2, trainable=True)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    T = tf.sigmoid(tf.matmul(x, self.W_T) + self.b_T, name=\"transform_gate\")\n",
        "    H = tf.nn.relu(tf.matmul(x, self.W) + self.b, name=\"activation\")\n",
        "    C = tf.subtract(1.0, T, name=\"carry_gate\")\n",
        "    \n",
        "    return tf.add(tf.multiply(H, T), tf.multiply(x, C), \"y\")\n",
        "\n",
        "class text_video:\n",
        "  # second input model\n",
        "  input2 = Input(shape=(250,388), name = 'Video_input')\n",
        "  highway2 = Highway()(input2)\n",
        "  highway3 = Highway()(highway2)\n",
        "  highway4 = Highway()(highway3)\n",
        "  # dense7 = Dense(200)(highway4)\n",
        "  dense2 = Dense(250)(highway4)\n",
        "\n",
        "  input3 = Input(shape = (250,5100), name = 'Text_input')\n",
        "  dense4 = Dense(1000)(input3)\n",
        "  dense5 = Dense(500)(dense4)\n",
        "  # dense6 = Dense(250)(dense5)\n",
        "  dense3 = Dense(250)(dense5)\n",
        "  # merge input models\n",
        "  merge = concatenate([dense2,dense3], axis = 1)\n",
        "  # interpretation model\n",
        "  lstm = LSTM(128, dropout = 0.2, recurrent_dropout = 0.2)(merge)\n",
        "  output = Dense(1, activation='sigmoid')(lstm)\n",
        "  model = Model(inputs=[input2, input3], outputs=output)\n",
        "  # summarize layers\n",
        "  # print(model.summary())\n",
        "  # # plot graph\n",
        "  # plot_model(model)\n",
        "  def run_model(self):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.0001)\n",
        "    self.model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "    return self.model\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNelM26pDEiN"
      },
      "source": [
        "#Text+Audio+Video\n",
        "\n",
        "class Highway(layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Highway, self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    n_sentences = input_shape[1]\n",
        "    n_features = input_shape[2]\n",
        "    carry_bias = keras.initializers.Constant(value=-1.0)\n",
        "    random_dist = keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
        "\n",
        "    carry_bias_2 = keras.initializers.Constant(value= 0.1)\n",
        "\n",
        "    self.W_T = self.add_weight(shape=(n_features, n_features),initializer = random_dist,trainable=True)\n",
        "    self.b_T = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias, trainable=True)\n",
        "   \n",
        "    self.W = self.add_weight(shape=( n_features, n_features),initializer = random_dist, trainable=True)\n",
        "    self.b = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias_2, trainable=True) \n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    T = tf.sigmoid(tf.matmul(x, self.W_T) + self.b_T, name=\"transform_gate\")\n",
        "    H = tf.nn.relu(tf.matmul(x, self.W) + self.b, name=\"activation\")\n",
        "    C = tf.subtract(1.0, T, name=\"carry_gate\")\n",
        "    \n",
        "    return tf.add(tf.multiply(H, T), tf.multiply(x, C), \"y\")\n",
        "\n",
        "class text_vieo_audio:\n",
        "  # first input model\n",
        "  input1 = Input(shape=(250,74), name = 'Audio_input')\n",
        "  highway1 = Highway()(input1)\n",
        "  highway5 = Highway()(highway1)\n",
        "  highway6 = Highway()(highway5)\n",
        "  dense1 = Dense(74)(highway6)\n",
        "\n",
        "  # second input model\n",
        "  input2 = Input(shape=(250,388), name = 'Video_input')\n",
        "  highway2 = Highway()(input2)\n",
        "  highway3 = Highway()(highway2)\n",
        "  highway4 = Highway()(highway3)\n",
        "  dense7 = Dense(200)(highway4)\n",
        "  dense2 = Dense(74)(dense7)\n",
        "\n",
        "  input3 = Input(shape = (250,5100), name = 'Text_input')\n",
        "  dense4 = Dense(1000)(input3)\n",
        "  dense5 = Dense(500)(dense4)\n",
        "  dense6 = Dense(250)(dense5)\n",
        "  dense3 = Dense(74)(dense6)\n",
        "  # merge input models\n",
        "  merge = concatenate([dense1,dense2,dense3], axis = 1)\n",
        "  lstm = LSTM(128, dropout = 0.2, recurrent_dropout = 0.2)(merge)\n",
        "  # bidire = Bidirectional(lstm)\n",
        "  output = Dense(1, activation='sigmoid')(lstm)\n",
        "  model = Model(inputs=[input1, input2, input3], outputs=output)\n",
        "  def run_model(self):\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.0001)\n",
        "    self.model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "    return self.model\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_location = \"dev_data\"\n",
        "test_location = \"test_data\"\n",
        "train_location = \"train_data\""
      ],
      "metadata": {
        "id": "cRzbDP68lIVq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devData = np.array(pd.read_csv('/content/drive/MyDrive/DAIC/dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "testData = np.array(pd.read_csv('/content/drive/MyDrive/DAIC/full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "trainData = np.array(pd.read_csv('/content/drive/MyDrive/DAIC/train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]"
      ],
      "metadata": {
        "id": "bgy7qNVwlJ5D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHCKLXfLx3Kc",
        "outputId": "a1800ac5-2d90-44fc-ffe1-48ddd2b0a411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = np.concatenate((devData, np.concatenate((testData, trainData))))\n",
        "\n",
        "gc.collect()      \n",
        "max_num_words = 17\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/DAIC/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "def checkDataPointExistence(patientID, split):\n",
        "  for i in split:\n",
        "    if(patientID == i[0]):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def getData(patientID, location):\n",
        "  # print(\"PatientID: \" + str(int(patientID)))\n",
        "  retData = [int(patientID)]\n",
        "  textD = getTextData(patientID, location)\n",
        "  audioD = getAudioData(patientID, location, textD)\n",
        "  videoD = getVideoData(str(int(patientID)), location, textD)\n",
        "  # patientD = np.concatenate((textD, audioD, videoD), axis = 1)\n",
        "  # print(\"Final Patient Data: \" + str(patientD.shape))\n",
        "  return textD,audioD,videoD\n",
        "\n",
        "def getTextData(patientID, location):\n",
        "  fileName = \"/content/drive/MyDrive/DAIC/\"+ str(location) + \"/\" + str(int(patientID)) + \"_TRANSCRIPT.csv\"\n",
        "  file = np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))\n",
        "\n",
        "  # Remove All Utterences By Ellie:\n",
        "  for i in range(len(file)):\n",
        "    if(file[i][2] != 'Participant'):\n",
        "      np.delete(file, i)\n",
        "      i-=1\n",
        "\n",
        "  # Remove Speaker Columnn\n",
        "  file = np.delete(file, 2, 1)\n",
        "  \n",
        "  # Convert Text Into Word Vectors:\n",
        "  w2vs = np.zeros((1, max_num_words*300))\n",
        "  for i in range(len(file)):\n",
        "    sentence = file[i][2]\n",
        "    w2v = returnWordToVec(sentence)\n",
        "    w2vs = np.concatenate((w2vs, w2v), axis = 0)\n",
        "  w2vs = np.delete(w2vs, 0, 0)  \n",
        "\n",
        "  # Delete Sentences and Replace With W2Vs\n",
        "  file = np.delete(file, 2, 1)\n",
        "  file = np.concatenate((file, w2vs), axis = 1)\n",
        "  return file\n",
        "\n",
        "def remove_StopWords(sentence):\n",
        "    filtered_sentence = [] \n",
        "    for w in sentence: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence\n",
        "\n",
        "def returnWordToVec(sentence):\n",
        "  global max_num_words, stop_words, model\n",
        "  sentence = str(sentence).split(\" \")\n",
        "  sentence = remove_StopWords(sentence)\n",
        "  index_word = 0\n",
        "  wordMatrix = np.zeros(max_num_words*300)\n",
        "  for j in range(min(max_num_words, len(sentence))):\n",
        "    try:\n",
        "      word = sentence[j]\n",
        "      if(word[0] == '<'):\n",
        "        if(word.find('>')!=-1):\n",
        "          word = word[1:-1]\n",
        "        else:\n",
        "          word = word[1:]\n",
        "      else:\n",
        "        if(word.find('>')!=-1):\n",
        "          word = word[0:-1]\n",
        "      ss = np.array(model[word])\n",
        "      wordMatrix[index_word*300:(index_word+1)*300] = ss\n",
        "      index_word+=1\n",
        "    except Exception as e:\n",
        "      continue\n",
        "  wordMatrix = np.array(wordMatrix.reshape(1,-1))\n",
        "  return wordMatrix\n",
        "\n",
        "def audioDataHelper(X):\n",
        "    for i in range(X.shape[0]):\n",
        "        if(X[i,1] == 0):\n",
        "            X[i,0] = 0\n",
        "            for j in range(7):\n",
        "                X[i,j+1] = 0\n",
        "    X = np.array(X)\n",
        "    return X\n",
        "    \n",
        "def getAudioData(patientID, location, textD):\n",
        "  fileName = \"/content/drive/MyDrive/DAIC/\"+ str(location) + \"/\" + str(int(patientID)) + \"_COVAREP.csv\"\n",
        "  data = pd.read_csv(fileName,header = None)\n",
        "  data = data.iloc[:,:].values\n",
        "  data = audioDataHelper(data)\n",
        "  # print(\"Audio Raw Data:\" + str(data.shape))\n",
        "  sentenceDatas = []\n",
        "  for sentence in textD:\n",
        "    sentenceStartime = sentence[0]\n",
        "    sentenceEndTime = sentence[1]\n",
        "    startIndex = math.floor(sentenceStartime/0.01)\n",
        "    endIndex = math.ceil(sentenceEndTime/0.01)\n",
        "    # print(\"Start Time: \" + str(startIndex))\n",
        "    # print(\"End Time: \" + str(endIndex))\n",
        "    sentenceData = data[startIndex: endIndex]\n",
        "    sentenceData = np.average(sentenceData, axis = 0)\n",
        "    # This might be a possible error\n",
        "    sentenceData = np.array(sentenceData.reshape(1, -1))\n",
        "    sentenceDatas.append(sentenceData)\n",
        "  \n",
        "  sentenceDatas = np.array(sentenceDatas)\n",
        "  sentenceDatas = np.reshape(sentenceDatas, (textD.shape[0],-1))\n",
        "  # print(\"Audio Final Data:\" + str(sentenceDatas.shape))\n",
        "\n",
        "  return sentenceDatas\n",
        "\n",
        "def getVideoDataHelper(patientID, location):\n",
        "  root = \"/content/drive/MyDrive/DAIC/\"+ str(location) + \"/\" \n",
        "  file1 = root + (patientID)+\"_CLNF_AUs.txt\"\n",
        "  file2 = root + (patientID)+\"_CLNF_features.txt\"\n",
        "  file3 = root + (patientID)+\"_CLNF_features3D.txt\"\n",
        "  file4 = root + (patientID)+\"_CLNF_gaze.txt\"\n",
        "  file5 = root + (patientID)+\"_CLNF_hog.txt\"\n",
        "  file6 = root + (patientID)+\"_CLNF_pose.txt\"\n",
        "  data = processVideoData(file1)\n",
        "  data = np.concatenate((data, processVideoData(file2)), 1)\n",
        "  data = np.concatenate((data, processVideoData(file3)), 1)\n",
        "  data = np.concatenate((data, processVideoData(file4)), 1)\n",
        "  data = np.concatenate((data, processVideoData(file6)), 1)\n",
        "  # print(\"Video Raw Data:\" + str(data.shape))\n",
        "  return data\n",
        "\n",
        "def processVideoData(filename):\n",
        "  try:\n",
        "    data = pd.read_csv(filename,delimiter=',', dtype=float)\n",
        "    X = data.iloc[:,:].values\n",
        "    X = np.delete(X, 0, 1)\n",
        "    X = np.delete(X, 1, 1)\n",
        "  except:\n",
        "    # print(\"Video Data corrupt, fixing.\")\n",
        "    data = pd.read_csv(filename,delimiter=',')\n",
        "    X = data.iloc[:,:].values\n",
        "    X = np.delete(X, 0, 1)\n",
        "    X = np.delete(X, 1, 1)\n",
        "    for i in range(len(X)):\n",
        "        if(isinstance(X[i][5],str) or isinstance(X[i][7],str)):\n",
        "            X[i] = np.zeros((1, X.shape[1]))\n",
        "            # print(\"se\" , end = \" \")    \n",
        "  return X\n",
        "\n",
        "def getVideoData(patientID, location, textD):\n",
        "  data = getVideoDataHelper(patientID, location)\n",
        "  sentenceDatas = []\n",
        "  for sentence in textD:\n",
        "    sentenceStartime = sentence[0]\n",
        "    sentenceEndTime = sentence[1]\n",
        "    startIndex = math.floor(sentenceStartime/0.333)\n",
        "    endIndex = math.ceil(sentenceEndTime/0.333)\n",
        "    # print(\"Start Time: \" + str(startIndex))\n",
        "    sentenceData = data[startIndex: endIndex]\n",
        "    sentenceData = np.average(sentenceData, axis = 0)\n",
        "    # This might be a possible error\n",
        "    sentenceData = np.array(sentenceData.reshape(1, -1))\n",
        "    sentenceDatas.append(sentenceData)\n",
        "  \n",
        "  sentenceDatas = np.array(sentenceDatas)\n",
        "  sentenceDatas = np.reshape(sentenceDatas, (textD.shape[0],-1))\n",
        "  # print(\"Video Final Data:\" + str(sentenceDatas.shape))\n",
        "  return sentenceDatas\n",
        "\n",
        "# Xtrain = []\n",
        "Ytrain = []\n",
        "# Xtest = []\n",
        "Ytest = []\n",
        "\n",
        "\n",
        "audio_train = []\n",
        "video_train = []\n",
        "text_train = []\n",
        "\n",
        "audio_test = []\n",
        "video_test = []\n",
        "text_test = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for datapoint in dataset:\n",
        "  # print(datapoint[0])\n",
        "  if(checkDataPointExistence(datapoint[0], devData)):\n",
        "\n",
        "    # Data Point in Dev Set\n",
        "    text,audio,video = getData(datapoint[0], dev_location)\n",
        "    audio_train.append(audio)\n",
        "    video_train.append(video)\n",
        "    text_train.append(text)\n",
        "    # Xtest.append(data)\n",
        "    Ytrain.append(datapoint[1])\n",
        "    # print(data)\n",
        "  elif(checkDataPointExistence(datapoint[0], testData)):\n",
        "    # Data Point in Test Set\n",
        "    text,audio,video = getData(datapoint[0], test_location)\n",
        "    audio_test.append(audio)\n",
        "    video_test.append(video)\n",
        "    text_test.append(text)\n",
        "    # Xtest.append(data)\n",
        "    Ytest.append(datapoint[1])\n",
        "  elif(checkDataPointExistence(datapoint[0], trainData)):\n",
        "    # Data Point in Train Set\n",
        "    text,audio,video = getData(datapoint[0], train_location)\n",
        "    audio_train.append(audio)\n",
        "    video_train.append(video)\n",
        "    text_train.append(text)\n",
        "    # Xtest.append(data)\n",
        "    Ytrain.append(datapoint[1])\n",
        "\n",
        "def refactor(arr, size):\n",
        "  arrsize = arr.shape[0]\n",
        "  temp = np.zeros((size, arr.shape[1]))\n",
        "  for i in range(min(len(arr), size)):\n",
        "    temp[i] = arr[i]\n",
        "  return temp\n",
        "\n",
        "numberOfSentences = 250\n",
        "\n",
        "devData = []\n",
        "trainData = []\n",
        "testData = []\n",
        "gc.collect()\n",
        "\n",
        "for i in range(len(audio_train)):\n",
        "  audio_train[i] = refactor(audio_train[i], numberOfSentences)\n",
        "  video_train[i] = refactor(video_train[i], numberOfSentences)\n",
        "  text_train[i] = refactor(text_train[i], numberOfSentences)\n",
        "  # print(Xtrain[i].shape)\n",
        "\n",
        "\n",
        "for i in range(len(audio_test)):\n",
        "  audio_test[i] = refactor(audio_train[i], numberOfSentences)\n",
        "  video_test[i] = refactor(video_train[i], numberOfSentences)\n",
        "  text_test[i] = refactor(text_train[i], numberOfSentences)\n",
        "  # print(Xtest[i].shape)\n",
        "audio_test = np.array(audio_test)\n",
        "video_test = np.array(video_test)\n",
        "text_test = np.array(text_test)\n",
        "text_test = text_test[:,:,2:]\n",
        "\n",
        "audio_train = np.array(audio_train)\n",
        "video_train = np.array(video_train)\n",
        "text_train = np.array(text_train)\n",
        "text_train = text_train[:,:,2:]\n",
        "\n",
        "\n",
        "dataset = []\n",
        "gc.collect()\n",
        "\n",
        "print(audio_test.shape,video_test.shape,text_test.shape)\n",
        "print(audio_train.shape,video_train.shape,text_train.shape)\n",
        "# print(audio_dev.shape,video_dev.shape,text_dev.shape)\n",
        "\n",
        "Ytrain = np.array(Ytrain)\n",
        "Ytest = np.array(Ytest)\n",
        "\n",
        "\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "def upsample(X_train,Y_train):\n",
        "  X_train_0 = X_train[Y_train==0]\n",
        "  X_train_1 = X_train[Y_train==1]\n",
        "\n",
        "  Y_train_1 = Y_train[Y_train==1]\n",
        "  # print(Y_train_1.shape)\n",
        "  # print(X_train_1.shape)\n",
        "  size = X_train_0.shape[0] - X_train_1.shape[0]\n",
        "  X = []\n",
        "  Y = []\n",
        "  X_train = list(X_train)\n",
        "  Y_train = list(Y_train)\n",
        "  while(size>0):\n",
        "    size -= 1\n",
        "    index = np.random.randint(0,X_train_1.shape[0]-1)\n",
        "    leave_index = np.random.randint(0,len(X_train)-1)\n",
        "    X_add = X_train_1[index]\n",
        "    X_leave = X_train[leave_index]\n",
        "\n",
        "    Y_add = Y_train_1[index]\n",
        "    Y_leave = Y_train[leave_index]\n",
        "\n",
        "    X_train[leave_index] = X_add\n",
        "    X_train.append(X_leave)\n",
        "\n",
        "    Y_train[leave_index] = Y_add\n",
        "    Y_train.append(Y_leave)\n",
        "\n",
        "\n",
        "  X_train = np.array(X_train)\n",
        "  Y_train = np.array(Y_train)\n",
        "  return X_train,Y_train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "audio_train = np.nan_to_num(audio_train)\n",
        "video_train = np.nan_to_num(video_train)\n",
        "text_train = np.nan_to_num(text_train)\n",
        "\n",
        "audio_train, _ = upsample(audio_train,Ytrain)\n",
        "video_train, _ = upsample(video_train,Ytrain)\n",
        "text_train, Ytrain = upsample(text_train,Ytrain)\n",
        "\n",
        "print(audio_train.shape)\n",
        "print(video_train.shape)\n",
        "print(text_train.shape)\n",
        "print(Ytrain.shape)\n",
        "\n",
        "for i in range(audio_train.shape[0]):\n",
        "  audio_train[i] = sklearn.preprocessing.normalize(audio_train[i])\n",
        "  video_train[i] = sklearn.preprocessing.normalize(video_train[i])\n",
        "  text_train[i] = sklearn.preprocessing.normalize(text_train[i])\n",
        "\n",
        "\n",
        "print(Ytest.shape)\n",
        "\n",
        "audio_test = np.nan_to_num(audio_test)\n",
        "video_test = np.nan_to_num(video_test)\n",
        "text_test = np.nan_to_num(text_test)\n",
        "\n",
        "\n",
        "for i in range(audio_test.shape[0]):\n",
        "  audio_test[i] = sklearn.preprocessing.normalize(audio_test[i])\n",
        "  video_test[i] = sklearn.preprocessing.normalize(video_test[i])\n",
        "  text_test[i] = sklearn.preprocessing.normalize(text_test[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-a4e6c35f0458>:128: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = np.concatenate((data, processVideoData(file2)), 1)\n",
            "<ipython-input-9-a4e6c35f0458>:129: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  data = np.concatenate((data, processVideoData(file3)), 1)\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret = um.true_divide(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(47, 250, 74) (47, 250, 388) (47, 250, 5100)\n",
            "(142, 250, 74) (142, 250, 388) (142, 250, 5100)\n",
            "(200, 250, 74)\n",
            "(200, 250, 388)\n",
            "(200, 250, 5100)\n",
            "(200,)\n",
            "(47,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1wTtgg4yOeK",
        "outputId": "f8748a68-6b23-4c71-a4c7-938f51305d4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def Thresholding(Y_pred, threshold):\n",
        "  Y_pred2 = []\n",
        "  print(\"Y_pred: \", Y_pred.shape)\n",
        "  for i in range(len(Y_pred)):\n",
        "    if(Y_pred[i] < threshold):\n",
        "      Y_pred2.append(0)\n",
        "    else:\n",
        "      Y_pred2.append(1)\n",
        "\n",
        "  return np.array(Y_pred2)\n",
        "\n",
        "gc.collect()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbT2jzhhu4Om",
        "outputId": "70077183-7200-4928-f501-5789a97a6465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"--------------------------------------AUDIO + TEXT (W GATING) SENTENCE LEVEL-------------------------------------------------\")\n",
        "model1 = text_audio()\n",
        "model = model1.run_model()\n",
        "\n",
        "model.fit([audio_train,text_train],Ytrain, validation_split = 0.2, callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='min',\n",
        "        baseline=None, restore_best_weights=True),epochs=50, batch_size = 137)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "pred = model.predict([audio_test,text_test])\n",
        "pred2 = model.predict([audio_train,text_train])\n",
        "\n",
        "print(classification_report(Ytest,Thresholding(pred,0.5)))\n",
        "print(\"TRAINING ACCCCCC        \" , classification_report(Ytrain,Thresholding(pred2,0.5)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.6)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.4)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.3)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.7)))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------AUDIO + TEXT (W GATING) SENTENCE LEVEL-------------------------------------------------\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 19s 3s/step - loss: 0.6934 - val_loss: 0.6924\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6896 - val_loss: 0.6915\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6864 - val_loss: 0.6907\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 12s 3s/step - loss: 0.6826 - val_loss: 0.6897\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6790 - val_loss: 0.6886\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 13s 4s/step - loss: 0.6739 - val_loss: 0.6881\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6701 - val_loss: 0.6873\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6643 - val_loss: 0.6844\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6587 - val_loss: 0.6807\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6518 - val_loss: 0.6775\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6457 - val_loss: 0.6747\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 12s 3s/step - loss: 0.6363 - val_loss: 0.6711\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6272 - val_loss: 0.6671\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6164 - val_loss: 0.6607\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.6047 - val_loss: 0.6521\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.5906 - val_loss: 0.6413\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.5772 - val_loss: 0.6276\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.5601 - val_loss: 0.6160\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 21s 4s/step - loss: 0.5435 - val_loss: 0.6076\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.5300 - val_loss: 0.6005\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.5158 - val_loss: 0.5957\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.5035 - val_loss: 0.5892\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4923 - val_loss: 0.5807\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4785 - val_loss: 0.5694\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 12s 3s/step - loss: 0.4681 - val_loss: 0.5507\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4561 - val_loss: 0.5291\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4467 - val_loss: 0.5176\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4374 - val_loss: 0.5114\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4270 - val_loss: 0.5100\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4167 - val_loss: 0.5128\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4094 - val_loss: 0.5146\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.4043 - val_loss: 0.5161\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3997 - val_loss: 0.5123\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3946 - val_loss: 0.5068\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3884 - val_loss: 0.5003\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3835 - val_loss: 0.4915\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3835 - val_loss: 0.4785\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3874 - val_loss: 0.4644\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3914 - val_loss: 0.4511\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 14s 3s/step - loss: 0.3857 - val_loss: 0.4389\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3778 - val_loss: 0.4405\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3697 - val_loss: 0.4564\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3699 - val_loss: 0.4767\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3676 - val_loss: 0.4949\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3667 - val_loss: 0.4967\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3654 - val_loss: 0.4837\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3633 - val_loss: 0.4631\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3643 - val_loss: 0.4487\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3634 - val_loss: 0.4482\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 11s 3s/step - loss: 0.3611 - val_loss: 0.4527\n",
            "2/2 [==============================] - 2s 414ms/step\n",
            "7/7 [==============================] - 6s 857ms/step\n",
            "Y_pred:  (47, 1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.33      0.43        33\n",
            "         1.0       0.24      0.50      0.33        14\n",
            "\n",
            "    accuracy                           0.38        47\n",
            "   macro avg       0.43      0.42      0.38        47\n",
            "weighted avg       0.50      0.38      0.40        47\n",
            "\n",
            "Y_pred:  (200, 1)\n",
            "TRAINING ACCCCCC                       precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.59      0.72       100\n",
            "         1.0       0.70      0.94      0.80       100\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.80      0.76      0.76       200\n",
            "weighted avg       0.80      0.77      0.76       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HXsKQ0kyc_0",
        "outputId": "1354f4b0-6548-47e9-96f9-0fe15d0e298e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"--------------------------------------VIDEO + TEXT (W GATING) SENTENCE LEVEL-------------------------------------------------\")\n",
        "model2 = text_video()\n",
        "model = model2.run_model()\n",
        "\n",
        "model.fit([video_train,text_train],Ytrain, validation_split = 0.2, callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='min',\n",
        "        baseline=None, restore_best_weights=True),epochs=50, batch_size = 137)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "pred = model.predict([video_test,text_test])\n",
        "pred2 = model.predict([video_train,text_train])\n",
        "\n",
        "print(classification_report(Ytest,Thresholding(pred,0.5)))\n",
        "print(\"TRAINING ACCCCCC        \" , classification_report(Ytrain,Thresholding(pred2,0.5)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.6)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.4)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.3)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.7)))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------VIDEO + TEXT (W GATING) SENTENCE LEVEL-------------------------------------------------\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 20s 4s/step - loss: 0.6933 - val_loss: 0.6924\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6889 - val_loss: 0.6912\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6856 - val_loss: 0.6899\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.6820 - val_loss: 0.6882\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6784 - val_loss: 0.6863\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6750 - val_loss: 0.6846\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 17s 5s/step - loss: 0.6694 - val_loss: 0.6827\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6640 - val_loss: 0.6810\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6581 - val_loss: 0.6793\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6510 - val_loss: 0.6768\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.6432 - val_loss: 0.6729\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6336 - val_loss: 0.6675\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6233 - val_loss: 0.6612\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.6110 - val_loss: 0.6554\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5978 - val_loss: 0.6478\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5823 - val_loss: 0.6382\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.5691 - val_loss: 0.6296\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5535 - val_loss: 0.6245\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5383 - val_loss: 0.6176\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.5236 - val_loss: 0.6032\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5093 - val_loss: 0.5869\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4964 - val_loss: 0.5737\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4850 - val_loss: 0.5647\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 17s 4s/step - loss: 0.4718 - val_loss: 0.5581\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4599 - val_loss: 0.5634\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4477 - val_loss: 0.5734\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4388 - val_loss: 0.5632\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4284 - val_loss: 0.5448\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4175 - val_loss: 0.5232\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4099 - val_loss: 0.5084\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.4026 - val_loss: 0.5085\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3963 - val_loss: 0.5095\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3918 - val_loss: 0.5055\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3872 - val_loss: 0.4948\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3813 - val_loss: 0.4932\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3772 - val_loss: 0.4982\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3715 - val_loss: 0.5000\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3701 - val_loss: 0.5111\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3691 - val_loss: 0.5326\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3667 - val_loss: 0.5519\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 17s 4s/step - loss: 0.3647 - val_loss: 0.5591\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3660 - val_loss: 0.5328\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3664 - val_loss: 0.4934\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3674 - val_loss: 0.4507\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3618 - val_loss: 0.4240\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3589 - val_loss: 0.4180\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3603 - val_loss: 0.4316\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3611 - val_loss: 0.4608\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3576 - val_loss: 0.4992\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 14s 4s/step - loss: 0.3550 - val_loss: 0.5446\n",
            "2/2 [==============================] - 2s 509ms/step\n",
            "7/7 [==============================] - 6s 882ms/step\n",
            "Y_pred:  (47, 1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.36      0.46        33\n",
            "         1.0       0.25      0.50      0.33        14\n",
            "\n",
            "    accuracy                           0.40        47\n",
            "   macro avg       0.44      0.43      0.40        47\n",
            "weighted avg       0.52      0.40      0.42        47\n",
            "\n",
            "Y_pred:  (200, 1)\n",
            "TRAINING ACCCCCC                       precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.57      0.69       100\n",
            "         1.0       0.68      0.92      0.78       100\n",
            "\n",
            "    accuracy                           0.74       200\n",
            "   macro avg       0.78      0.74      0.74       200\n",
            "weighted avg       0.78      0.74      0.74       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ0Bx0rdy75t",
        "outputId": "bc3e77af-5546-44f9-a5f5-e0613bef2638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"--------------------------------------AUDIO + VIDEO + TEXT (W GATING) SENTENCE LEVEL-------------------------------------------------\")\n",
        "model3 = text_vieo_audio()\n",
        "model = model3.run_model()\n",
        "\n",
        "model.fit([audio_train,video_train,text_train],Ytrain, validation_split = 0.2, callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='min',\n",
        "    baseline=None, restore_best_weights=True),epochs=50, batch_size = 137)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "pred = model.predict([audio_test,video_test,text_test])\n",
        "pred2 = model.predict([audio_train,video_train,text_train])\n",
        "\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.5)))\n",
        "print(\"TRAINING ACCCCCC        \" , classification_report(Ytrain,Thresholding(pred2,0.5)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.6)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.4)))\n",
        "print(classification_report(Ytest,Thresholding(pred,0.3)))\n",
        "# print(classification_report(Ytest,Thresholding(pred,0.7)))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------AUDIO + VIDEO + TEXT (W GATING) SENTENCE LEVEL-------------------------------------------------\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 22s 4s/step - loss: 0.6934 - val_loss: 0.6922\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6903 - val_loss: 0.6910\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6871 - val_loss: 0.6898\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6844 - val_loss: 0.6889\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6810 - val_loss: 0.6880\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6766 - val_loss: 0.6874\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6727 - val_loss: 0.6868\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6683 - val_loss: 0.6857\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6626 - val_loss: 0.6842\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6549 - val_loss: 0.6823\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6479 - val_loss: 0.6801\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6402 - val_loss: 0.6761\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6298 - val_loss: 0.6693\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6162 - val_loss: 0.6608\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.6031 - val_loss: 0.6486\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5861 - val_loss: 0.6319\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 17s 6s/step - loss: 0.5709 - val_loss: 0.6148\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5566 - val_loss: 0.6000\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5431 - val_loss: 0.5868\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5289 - val_loss: 0.5806\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.5064 - val_loss: 0.5950\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4924 - val_loss: 0.6069\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4815 - val_loss: 0.5781\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4630 - val_loss: 0.5391\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4494 - val_loss: 0.5170\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4385 - val_loss: 0.5193\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4229 - val_loss: 0.5577\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4159 - val_loss: 0.5913\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4121 - val_loss: 0.5641\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.4016 - val_loss: 0.5042\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3922 - val_loss: 0.4609\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3897 - val_loss: 0.4569\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3821 - val_loss: 0.4942\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 17s 5s/step - loss: 0.3757 - val_loss: 0.5549\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3741 - val_loss: 0.5730\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 17s 4s/step - loss: 0.3741 - val_loss: 0.5364\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3733 - val_loss: 0.4969\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3699 - val_loss: 0.4616\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3684 - val_loss: 0.4452\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3655 - val_loss: 0.4436\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3620 - val_loss: 0.4423\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3603 - val_loss: 0.4381\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3608 - val_loss: 0.4609\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3626 - val_loss: 0.5004\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3605 - val_loss: 0.4997\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3577 - val_loss: 0.5003\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3584 - val_loss: 0.5184\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3551 - val_loss: 0.6037\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3525 - val_loss: 0.6658\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 15s 4s/step - loss: 0.3542 - val_loss: 0.6075\n",
            "2/2 [==============================] - 3s 780ms/step\n",
            "7/7 [==============================] - 8s 944ms/step\n",
            "Y_pred:  (200, 1)\n",
            "TRAINING ACCCCCC                       precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.57      0.71       100\n",
            "         1.0       0.69      0.96      0.80       100\n",
            "\n",
            "    accuracy                           0.77       200\n",
            "   macro avg       0.81      0.76      0.76       200\n",
            "weighted avg       0.81      0.77      0.76       200\n",
            "\n",
            "Y_pred:  (47, 1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.27      0.37        33\n",
            "         1.0       0.23      0.50      0.31        14\n",
            "\n",
            "    accuracy                           0.34        47\n",
            "   macro avg       0.39      0.39      0.34        47\n",
            "weighted avg       0.46      0.34      0.35        47\n",
            "\n"
          ]
        }
      ]
    }
  ]
}